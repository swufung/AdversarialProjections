{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toy_manifold_projection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pnoqgWhoRzy"
      },
      "source": [
        "#**Toy Example: Manifold Projections**\n",
        "\n",
        "![notebook-result-illustration](https://raw.githubusercontent.com/swufung/AdversarialProjections/master/Toy-Examples/toy-manifold-illustration.PNG)\n",
        "\n",
        "*(The above is a screenshot from our [paper](https://arxiv.org/pdf/2008.02200.pdf). All data plotted here were generated from running this notebook.)*\n",
        "\n",
        "</br>\n",
        "\n",
        "**Created by:** Howard Heaton, Samy Wu Fung, Alex Tong Lin\n",
        "\n",
        "**Date Created:** May 2020\n",
        "\n",
        "**Date Updated:** November 11, 2020\n",
        "\n",
        "_Note_: Through the menu you can save a copy of this code (File --> Save a Copy) and run it online through your own Google account (or run it locally on your computer if you have Jupyter set up).\n",
        "\n",
        "\n",
        "**Overview**\n",
        "\n",
        "This notebook has been created to provide a simple example for practitioners to learn a new tool for solving inverse problems. Here the core task is recover a signal $u^\\star$ from a collection of noisy measurements. One approach is to design a sort of \"black box\" neural network that we feed in measurements and train to (hopefully) be able to reliably reconstruct signals. Another approach is to create an analytic model and assume $u^\\star$ is the solution to the optimization problem\n",
        "\n",
        "$$ \\min_{u\\in\\mathbb{R}^n} H(u) + J(u),$$\n",
        "\n",
        "where $H$ is the _fidelity term_ that measures compliance with measurements and $J$ is the _regularizer_ that encodes prior knowledge about typically signals $u^\\star$. In classical optimization, $J$ is chosen by hand, yielding results that are now often subpar compared to state-of-the-art machine learning. However, the classic methods possess solid theoretical justification and robustness, properties machine learning approaches often lack.\n",
        "\n",
        "The new tool we illustrate in this notebook is how to leverage available data to construct a data-driven $J$ that fuses the strengths of machine learning with the theory of classic optimization. A key ingredient in this approach is to apply the common assumption that most signals in practice exhibit redudancies (e.g., pictures  often have many pixel values that are correlated to each other), and can therefore be represent more efficiently in some lower dimensional space. This is to say that each $u^\\star$ is contained in a manifold $\\mathcal{M}$ of intrinsically low dimension. Ideally, we would recover a signal estimate in $\\mathcal{M}$.\n",
        "This requires solving a constrained optimization problem such as\n",
        "\n",
        "$$ \\min_{u\\in\\mathbb{R}^n} H(u) + \\delta_{\\mathcal{M}}(u) = \\min_{u\\in\\mathcal{M}} H(u).$$\n",
        "\n",
        "To solve this problem, optimization algorithms will iteratively project estimates on $\\mathcal{M}$. Unfortunately, if we only have a few samples of true signals $u^\\star \\in \\mathcal{M}$, we are unlikely to find a closed form expression for this. However, our [paper](https://arxiv.org/pdf/2008.02200.pdf) introduces the first (to our knowledge) approach to rigorously accomplish this task, illustrated on a toy example in this notebook.\n",
        "\n",
        "To achieve our goal of simplicity, we use a 2D example and shorten the code as much as possible (e.g., by removing the use of tensor dataloaders for batching). The notebook first consists of creating data (sampling from the manifold and generating samples to be projected onto the manifold). Then we define our neural network architecture and train the network to project the generated examples onto an approximation of the manifold. We say _approximation_ since we use only a discrete sampling of the manifold for learning and a finite number of iterations (as this resembles practical situations). After training, we use these projections to solve a basic optimization problem involving least squares. There we illustrate how projected gradient (using an analytic formula for the true manifold) compares to our adversarially projected gradient method. The only distinction between the two methods is the operator used to project onto the manifold. As we will see, the learned projection generalizes surprisingly well for the sparsely sampled manifold.\n",
        "\n",
        "\n",
        "**Data Creation**\n",
        "\n",
        "We first generate samples from true manifold $\\mathcal{M}$ given by the upper half circle of radius $0.75$ centered at $(2,0)$  so that each sample $u^\\star \\in \\mathcal{M}$ takes the form\n",
        "\n",
        "$u^\\star =\\left[\\begin{array}{c} 2 \\\\ 0 \\end{array}\\right] + \\dfrac{3}{4} \\left[\\begin{array}{c} \\cos(\\theta) \\\\ \\sin(\\theta) \\end{array}\\right]$ with $\\theta \\sim \\mbox{Uniform}[0,\\pi]$.\n",
        "\n",
        "We will restrict our interest of use for the projection operator $P_{\\mathcal{M}}$ to the region $[0,3]\\times [-0.5,1.5]\\subset \\mathbb{R}^2$. \n",
        "So, we generate random samples in $\\mathbb{R}^2$ to project onto $\\mathcal{M}$ using\n",
        "\n",
        "$u^1 = \\left[\\begin{array}{c} \\xi_1 \\\\ \\xi_2 \\end{array}\\right]$\n",
        "with\n",
        "$\\xi_1 \\sim \\mbox{Uniform}[0,3]$ \n",
        "and \n",
        "$\\xi_2 \\sim \\mbox{Uniform}[-0.5,1.5]$.\n",
        "\n",
        "Running the following code cell creates the numpy arrays ```u_gen``` and ```u_true```, and then plots both of these in the plane.\n",
        "\n",
        "_Note_: We believe it is helpful to include a small amount of Gaussian noise to the samples in $u^1$. This ensures the distance function estimate is reasonable within a neighborhood of the points, and has been observed to yield improved results in our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4MoFa9fkhls"
      },
      "source": [
        "import matplotlib\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn              as nn\n",
        "import torch.optim           as optim\n",
        "import matplotlib.pyplot     as plt\n",
        "import numpy                 as np\n",
        "from   mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from   torch.utils.data        import Dataset, TensorDataset, DataLoader\n",
        "import time\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "device = \"cpu\" \n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# Sampling and Batch Sizes\n",
        "#-----------------------------------------------------------------\n",
        "n_samples_man = 40  \n",
        "n_samples     = 500 \n",
        "#-----------------------------------------------------------------\n",
        "# Manifold Data\n",
        "#-----------------------------------------------------------------\n",
        "u_true = torch.ones(n_samples_man, 2).to(device)\n",
        "for i in range(n_samples_man):\n",
        "    theta = torch.rand(1)\n",
        "    r     = 1.5 \n",
        "    u_true[i,0] = 2 + 0.5 * r * np.cos( 3.1415 * theta)\n",
        "    u_true[i,1] = 0.5 * r * np.sin( 3.1415 * theta)\n",
        "#-----------------------------------------------------------------\n",
        "# Measurement Data\n",
        "#-----------------------------------------------------------------\n",
        "u_gen       = 3 * torch.rand(n_samples, 2).to(device)  \n",
        "u_gen[:,1] -= 0.5\n",
        "u_gen[:,0] += 0\n",
        "u1          = u_gen.to(device)\n",
        "#-----------------------------------------------------------------\n",
        "# Plot Manifold Samples\n",
        "#-----------------------------------------------------------------\n",
        "x_min = 0\n",
        "x_max = 3\n",
        "y_min = -0.5\n",
        "y_max = 2.5\n",
        "\n",
        "\n",
        "def plot_distributions(dist1, dist2, index):\n",
        "  fig = plt.figure()\n",
        "  plt.scatter(dist1[:,0],dist1[:,1], c='r', alpha=0.3)\n",
        "  plt.scatter(dist2[:,0],dist2[:,1], c='b', alpha=0.3)\n",
        "  plt.ylim(y_min, y_max);\n",
        "  plt.xlim(x_min, x_max)\n",
        "  plt.legend(['manifold samples', 'generated samples'], loc='upper left',)\n",
        "  title_str = 'Manifold Samples' + str(index)\n",
        "  plt.title(title_str)\n",
        "  plt.show()\n",
        "  plt.close(fig)\n",
        "  plt.close(plt.gcf())\n",
        "\n",
        "plot_distributions(u_true, u_gen, 1)\n",
        "\n",
        "def get_prefix(ctr): # Prefix for saving file names\n",
        "    if ctr < 10:\n",
        "        return '00000'\n",
        "    elif ctr < 100:\n",
        "        return '0000'\n",
        "    elif ctr < 1000:\n",
        "        return '000'\n",
        "    elif ctr < 10000:\n",
        "        return '00'\n",
        "    elif ctr < 100000:\n",
        "        return '0'\n",
        "    else:\n",
        "        return '' \n",
        "\n",
        "filename = './manifold.csv'\n",
        "with open(filename, 'w') as f: \n",
        "  f.write('a,b\\n')\n",
        "  for file_ctr in range(u_true.shape[0]):     \n",
        "    f.write('%0.5e,%0.5e\\n' % (u_true[file_ctr, 0], u_true[file_ctr, 1]))   \n",
        "\n",
        "filename = './p_' + get_prefix(1) + str(1) + '.csv'\n",
        "with open(filename, 'w') as f: \n",
        "  f.write('a,b\\n')\n",
        "  for file_ctr in range(u_gen.shape[0]):     \n",
        "    f.write('%0.5e,%0.5e\\n' % (u_gen[file_ctr, 0], u_gen[file_ctr, 1])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clQjnBLUGSwg"
      },
      "source": [
        "**Network Definition**\n",
        "\n",
        "The network structure used follows the ideas from the [GroupSort paper](http://proceedings.mlr.press/v97/anil19a/anil19a.pdf), which we highly recommend reading. In short, our neural net structure is 1-Lipschitz with respect to its inputs and (effectively) gradient norm preserving with respect to the training weights, which yields wonderful stability and the ability to approximate any 1-Lipschitz function arbitrarily well (with a sufficiently large network). In addition, we use a convex combination with the identity operator during feed forward operations, which acts similarly to a [ResNet idea](https://arxiv.org/pdf/1805.07477.pdf) and provides further stability (allowing us to easily train deep networks). Choosing $\\beta < 1$ is not necessary in our simple problem. But, we note this still works fine (e.g., using $\\beta=0.75$ gives roughly the same final result) and may find good use on other problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiGJ0ODzGAqn"
      },
      "source": [
        "class distance_estimator(nn.Module):\n",
        "    def __init__(self, n_feats, n_hid_params, hidden_layers, n_projs, device, beta=0.5):\n",
        "        super().__init__()        \n",
        "        self.hidden_layers = hidden_layers  # number of hidden layers in the network\n",
        "        self.n_projs       = n_projs        # number of projections to use for weights onto Steiffel manifold\n",
        "        self.beta          = beta           # scalar in (0,1) for stabilizing feed forward operations\n",
        "        \n",
        "        # Intialize initial, middle, and final layers\n",
        "        self.fc_one = torch.nn.Linear(n_feats, n_hid_params, bias=True )\n",
        "        self.fc_mid = nn.ModuleList([torch.nn.Linear(n_hid_params, n_hid_params, bias=True ) for i in range(self.hidden_layers)])\n",
        "        self.fc_fin = torch.nn.Linear(n_hid_params, 1, bias=True )\n",
        "\n",
        "        # Normalize weights (helps ensure stability with learning rate)\n",
        "        self.fc_one.weight = nn.Parameter(self.fc_one.weight/torch.norm(self.fc_one.weight))\n",
        "        for i in range(self.hidden_layers):\n",
        "            self.fc_mid.weight = nn.Parameter(self.fc_mid[i].weight/torch.norm(self.fc_mid[i].weight))          \n",
        "        self.fc_fin.weight = nn.Parameter(self.fc_fin.weight/torch.norm(self.fc_fin.weight))\n",
        "                        \n",
        "    def forward(self, u):     \n",
        "        u = self.fc_one(u).sort(1)[0]                             # Apply first layer affine mapping\n",
        "        for i in range(self.hidden_layers):                       # Loop for each hidden layer                     \n",
        "          u = u + self.beta * (self.fc_mid[i](u).sort(1)[0] - u)  # Convex combo of u and sort(W*u+b)\n",
        "        u = self.fc_fin(u)                                        # Final layer is scalar (no need to sort)        \n",
        "        J = torch.abs(u)        \n",
        "        return J\n",
        "                   \n",
        "    \n",
        "    def project_weights(self):\n",
        "        self.fc_one.weight.data = self.proj_Stiefel(self.fc_one.weight.data, self.n_projs)\n",
        "        for i in range(self.hidden_layers):\n",
        "            self.fc_mid[i].weight.data = self.proj_Stiefel(self.fc_mid[i].weight.data, self.n_projs)\n",
        "        self.fc_fin.weight.data = self.proj_Stiefel(self.fc_fin.weight.data, self.n_projs)           \n",
        "            \n",
        "    def proj_Stiefel(self, Ak, proj_iters):  # Project to closest orthonormal matrix\n",
        "        n = Ak.shape[1]\n",
        "        I = torch.eye(n)\n",
        "        for k in range(proj_iters):\n",
        "            Qk = I - Ak.permute(1, 0).matmul(Ak)\n",
        "            Ak = Ak.matmul(I + 0.5 * Qk)\n",
        "        return Ak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFM4Gh8aoVyU"
      },
      "source": [
        "**Training Setup**\n",
        "\n",
        "Training consists of learning a projection operator $P_{\\mathcal{M}}$. This is an _intensive_ process. Once training is complete, we obtain a network $\\phi:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$ such that (hopefully)\n",
        "\n",
        "$ \\phi(u) \\approx P_{\\mathcal{M}}(u), \\ \\ \\ \\mbox{for all} \\ u \\in \\mathcal{M}.$\n",
        "\n",
        "Here\n",
        "\n",
        "$\\phi(u) = (T_K\\circ T_{K-1}\\circ\\cdots \\circ T_1)(u)$\n",
        "\n",
        "where\n",
        "\n",
        "$T_k(u) = (1-\\beta) u + \\beta \\sigma (W^ku + c^k)$, for $k\\in[K],$\n",
        "\n",
        "$\\sigma$ is the sorting operation, and $W^k$ and $c^k$ are a matrix and vector, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvvkPV2zuzMO"
      },
      "source": [
        "n_hid_params   = 10    # Parameters in each hidden layer (10)\n",
        "n_hid_layers   = 10    # Total hidden layers (10)\n",
        "n_projs        = 2     # Number of projections to Stiefel manifold (2)\n",
        "beta           = 1.0   # Relaxation parameter in network feed forward operations (1.0)\n",
        "n_feats        = 2     # Our signals are in R^2\n",
        "netD           = distance_estimator(n_feats, n_hid_params, n_hid_layers, n_projs, device, beta=beta)\n",
        "max_epochs     = int(1e7)\n",
        "learning_rate  = 2e-3     # (2e-3)\n",
        "optimizer      = optim.SGD(netD.parameters(), lr=learning_rate)\n",
        "drop_freq      = 1.0e5    # Frequency for decaying step sizes\n",
        "decay_rate     = 1.0      # Step size decay proportion\n",
        "prog_threshold = 1.0e-4   # Determines when to perform generator update\n",
        "eta_threshold  = 0.75     # Minimum value of eta allowed to perform generator update\n",
        "man_thresh     = 1.0e-2\n",
        "n_steps        = 20\n",
        "hist_dist      = np.zeros(n_steps)\n",
        "hist_eta       = np.zeros(n_steps)\n",
        "checkpt_path   = './models/'\n",
        "\n",
        "def gamma(k): # Relaxation parameter used for Halpern updates\n",
        "    return (1 + k) ** (-1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DahoGiZgvDcZ"
      },
      "source": [
        "**Training**\n",
        "\n",
        "Here we are trying the maximize the difference between our discriminator's values on the true data and the generated data. It is important to note that we train using a parameter $\\eta$ described in the preprint. In short, this $\\eta \\in [0,1]$ gives an indication of the proportion of samples that we are able to detect outside the manifold. So, for example, $\\eta = 0.34$ can be interpret to mean that our network detects that $34\\%$ of the points are outside the manifold. If the samples are entirely outside the manifold, then we would ideally obtain $\\eta\\approx 1.0$ during training. So, as training progresses, the $\\eta$ parameter will increase and we can use this to determine how \"close\" we are to obtaining our distance function $d_{\\mathcal{M}}$ estimate. Moreover, because $\\eta$ is restricted to the interval $[0,1]$, we can use the same stopping criterion for *all* the training, i.e., this is an absolute metric. (This is incredibly convenient since it is difficult to know how close one is to solving the problem looking only at the distance estimates...)\n",
        "\n",
        "Note: The fact that the network is 1-Lipschitz makes is so we can have some insight into the choice of step-size to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yoiCuvsoXTK",
        "scrolled": false
      },
      "source": [
        "#--------------------------------------\n",
        "#  Initialize Training\n",
        "#--------------------------------------\n",
        "k        = 1  # defines the index for {u^k}\n",
        "ave_diff = 0\n",
        "ctr      = 1  # inner loop for solving the k-th minimization problem\n",
        "eta      = 0.0\n",
        "dist_est = 0.0\n",
        "dist_man = 0.0\n",
        "loss     = 0.0\n",
        "eta_prev = 0.0\n",
        "uk       = u_gen\n",
        "\n",
        "print(netD) \n",
        "#------------------------------------------------------------\n",
        "# Identify if stopping criteria are met for current step\n",
        "#------------------------------------------------------------\n",
        "def stop_crit_met(epoch, ave_diff, eta, prog_thresh, eta_thresh, ctr, k, dist_man):    \n",
        "    if epoch%1000 != 0:           # Only update once every 100 epochs\n",
        "        return False\n",
        "    elif ctr > 100000:\n",
        "        return True \n",
        "    elif ave_diff > prog_thresh: # Require estimate progress to be small (close to optimal)\n",
        "        return False\n",
        "    elif ave_diff == 0:          # Require their to be forward progress \n",
        "        return False    \n",
        "    elif dist_man > man_thresh:\n",
        "        return False\n",
        "    elif eta < eta_thresh:\n",
        "        return False\n",
        "    return True\n",
        "#------------------\n",
        "# Training\n",
        "#------------------\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    if k < n_steps:\n",
        "        t0        = time.time()               \n",
        "        optimizer.zero_grad()  # Initialize gradient to zero\n",
        "        err_real     = torch.mean(netD(u_true))\n",
        "        # err_fake     = torch.mean(netD(uk))\n",
        "\n",
        "        # To artifically inflate P^k, we can include noise (following line)\n",
        "        err_fake     = torch.mean(netD(uk + 0.02 * torch.randn(uk.shape)))\n",
        "\n",
        "        err          = err_real - err_fake\n",
        "        soft_const   = 50 * torch.mean(netD(u_true) ** 2) # tau = 50, p = 2\n",
        "        loss         = err + soft_const\n",
        "        dist_est     = -0.001 * err.detach() + 0.999 * dist_est            \n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        dist_man = 0.01 * err_real.detach().numpy() + 0.99 * dist_man\n",
        "        netD.project_weights()\n",
        "        uk.requires_grad_(True)\n",
        "        Jout   = netD(uk)\n",
        "        nablaJ = torch.autograd.grad(outputs=Jout, inputs=uk, # take derivative w.r.t only inputs\n",
        "                                  grad_outputs=torch.ones(Jout.size()).to(device), only_inputs=True)[0]        \n",
        "        eta      = 0.001 * (torch.norm(nablaJ, p=2, dim=1) ** 2).mean().cpu().numpy() + 0.999 * eta\n",
        "        ave_diff = 0.001 * np.maximum(0, eta - eta_prev) + 0.999 * ave_diff\n",
        "        ctr += 1        \n",
        "        if ctr%100 == 0:\n",
        "            eta_prev = eta\n",
        "      \n",
        "        if stop_crit_met(epoch, ave_diff, eta, prog_threshold, eta_threshold, ctr, k, dist_man):\n",
        "          #------------------------------------\n",
        "          # Update distribution estimate uk\n",
        "          #------------------------------------                \n",
        "          beta = dist_est\n",
        "          uk.requires_grad_(True)\n",
        "          Jout   = netD(uk)\n",
        "          nablaJ = torch.autograd.grad(outputs=Jout, inputs=uk,   # Compute derivative w.r.t inputs\n",
        "                                    grad_outputs=torch.ones(Jout.size()).to(device), only_inputs=True)[0]                                                   \n",
        "          diag  = 0.25 * (beta + Jout)\n",
        "          uk    = gamma(k) * u1 + (1-gamma(k)) * (uk - diag * nablaJ)  \n",
        "          uk    = uk.detach()\n",
        "          k    += 1 # update outer iteration number\n",
        "          ctr   = 0 # Reset counter for stopping critera  \n",
        "          #--------------------------\n",
        "          # Save weights/step sizes\n",
        "          #--------------------------\n",
        "          state = {\n",
        "            'epoch': epoch,\n",
        "            'eta': eta,\n",
        "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "            'state_dict': netD.state_dict(),\n",
        "            'beta' : beta,\n",
        "            'ave_true': err_real.detach().cpu().numpy(),\n",
        "          }         \n",
        "          if not os.path.exists(os.path.dirname(checkpt_path)):\n",
        "              print(\"created path: \", checkpt_path)\n",
        "              os.makedirs(os.path.dirname(checkpt_path))\n",
        "          save_checkpt_str = checkpt_path + 'step_' + get_prefix(k) + str(k) + '.pth'          \n",
        "          torch.save(state, save_checkpt_str) \n",
        "          print('Saved Checkpoint:' + save_checkpt_str) \n",
        "        #------------------------\n",
        "        # Update drop frequency\n",
        "        #------------------------\n",
        "        #if ctr % drop_freq == 0 and ctr > 0:\n",
        "        #    optimizer.param_groups[0]['lr'] *= decay_rate        \n",
        "        \n",
        "        #------------------------\n",
        "        # Output training stats\n",
        "        #------------------------\n",
        "        if epoch % 1000 == 0:\n",
        "            print('[%d: %d/%d] d(uk): %.3e, d(man): %.3e, eta = %.3f, eta-diff = %0.3e, lr %.3e, timer %0.2f'\n",
        "                % (k, epoch, max_epochs, dist_est, dist_man, eta, ave_diff, optimizer.param_groups[0]['lr'], time.time() - t0))\n",
        "        #------------------------\n",
        "        # Plot training data\n",
        "        #------------------------\n",
        "        if ctr == 0:\n",
        "            plot_distributions(u_true.detach(), uk.detach(), k)        \n",
        "        # ------------------------\n",
        "        # Evaluate J on 2D grid\n",
        "        # ------------------------        \n",
        "        if ctr % 1000 == 0:\n",
        "            fig = plt.figure()\n",
        "            a = torch.linspace(0, 3, 200)\n",
        "            b = torch.linspace(2.5, -0.5, 200)\n",
        "            gridPts   = torch.stack(torch.meshgrid(a, b)).to(device)\n",
        "            gridShape = gridPts.shape[1:]\n",
        "            gridPts   = gridPts.reshape(2, -1).t()\n",
        "            disc_grid = netD(gridPts)\n",
        "            disc_grid = disc_grid.detach().cpu()\n",
        "            \n",
        "            plt.imshow(disc_grid.reshape(gridShape).t())\n",
        "            plt.clim(0,2.5)\n",
        "            plt.colorbar()\n",
        "            J_title_str = 'Manifold Distance Function Landscape:' + str(k)\n",
        "            fig.savefig('distance_landscape_' + get_prefix(epoch+1) + str(epoch+1) + '.png')\n",
        "            \n",
        "            plt.title(J_title_str)\n",
        "            plt.close(fig)\n",
        "            plt.close(plt.gcf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o4TSL1oiAmf"
      },
      "source": [
        "# Heat map\n",
        "The following snippet of code is used for creating a heat map type display of the distance function $d_{\\mathcal{M}}$ landscape (later compiled in LaTex for Figure 3b at the top o)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGIv_KHt5-Hw"
      },
      "source": [
        "#------------------------------------------------------------------\n",
        "# Save J_{theta}^1 heatmap data for Figure 3c (compiled in Latex)\n",
        "#------------------------------------------------------------------\n",
        "def heat_map(index):\n",
        "    netD        = distance_estimator(n_feats, n_hid_params, n_hid_layers, n_projs, device, beta=1.0);\n",
        "    load_path   = './models/step_' + get_prefix(index) + str(index) + '.pth'\n",
        "    checkpt     = torch.load(load_path, map_location=\"cpu\")\n",
        "    netD.load_state_dict(checkpt[\"state_dict\"])\n",
        "       \n",
        "    filename = './heat_map.txt'\n",
        "    img_size = 64\n",
        "    with open(filename, 'w') as f:    \n",
        "      heat_map = np.zeros((img_size, img_size))\n",
        "      for i in range(img_size):\n",
        "        f.write('{')\n",
        "        for j in range(img_size):\n",
        "          x         = x_min + j * (x_max - x_min) / img_size\n",
        "          y         = y_max + i * (y_min - y_max) / img_size\n",
        "          point     = np.array([[x,y]])\n",
        "          point_ten = torch.from_numpy(point).float()\n",
        "          dist      = 100 / 2.5 * netD(point_ten).detach().float()\n",
        "          if j == (img_size-1):\n",
        "            f.write('%0.1f' % (dist))\n",
        "          else:\n",
        "            f.write('%0.1f,' % (dist))\n",
        "        if i < img_size-1:\n",
        "          f.write('},\\n')\n",
        "        else:\n",
        "          f.write('},')        \n",
        "      print('Saved heat map.')\n",
        "\n",
        "heat_map(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT4ifBQNmVDY"
      },
      "source": [
        "\n",
        "# Application of Adversarial Projection to Toy Problem\n",
        "\n",
        "\n",
        "**Toy Example Problem**\n",
        "\n",
        "We consider the optimization problem\n",
        "\n",
        "$\\displaystyle \\min_{z\\in \\mathbb{R}^2} H(z) + \\delta_{\\mathcal{M}}(z) = \\min_{z \\in \\mathcal{M}} H(z),$\n",
        "\n",
        "where $A = [1\\ \\  2]$ and $b = 2$, and\n",
        "\n",
        "$H(u) := \\dfrac{1}{2}\\|Au-b\\|_2^2,$\n",
        "\n",
        "\n",
        "The iterative method we use to solve this problem is a relaxed version of projected gradient. For $\\tau \\in (0,1)$ and $\\kappa \\in (0, 2/\\mbox{Lip}(H))$, we use relaxed projected gradient updates of the form\n",
        "\n",
        "\n",
        "$\\ \\ \\ z^{t+1} = (1-\\tau)\\cdot z^t + \\tau\\cdot P_{\\mathcal{M}}(z^t - \\alpha \\nabla H(z^t))$.\n",
        "\n",
        "</br>\n",
        "\n",
        "Here we load the model parameters stored during our training to test our model on the problem above. We include two variations. One plot is of the trajectory using an analytic formula for the projection onto the manifold. The other uses our learned version, called adversarial projection. Note this could be implemented more efficiently, but the approach here is used to facilitate understanding of the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMoIDzsCnCOY"
      },
      "source": [
        "def proj_M_L2O(uk):\n",
        "    uk      = torch.from_numpy(uk)\n",
        "    u1      = uk  # create anchor point\n",
        "    n_steps = 20  # same number of steps as in training\n",
        "    netD    = distance_estimator(n_feats, n_hid_params, n_hid_layers, n_projs, device, beta=1.0);\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Begin push forward\n",
        "    #---------------------------------------------------------------------------\n",
        "    for k in range(2,n_steps+1):   \n",
        "        #-----------------------------------------------------------------------\n",
        "        # Load network state:\n",
        "        #-----------------------------------------------------------------------\n",
        "        resume_path = './models/step_' + get_prefix(k) + str(k) + '.pth'\n",
        "        checkpt     = torch.load(resume_path, map_location=\"cpu\")\n",
        "        netD.load_state_dict(checkpt[\"state_dict\"])\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute J(u_true), J(u_gen), eta\n",
        "        #-----------------------------------------------------------------------\n",
        "        uk.requires_grad_(True)\n",
        "        Jtrue = netD(u_true).mean()\n",
        "        Jgen  = netD(uk.float()).mean()\n",
        "        # take derivative w.r.t only inputs\n",
        "        nablaJ = torch.autograd.grad(outputs=Jgen, inputs=uk,\n",
        "                                  grad_outputs=torch.ones(Jgen.size()).to(\"cpu\"), only_inputs=True)[0]\n",
        "        if k > 1: # Update u^k --> u^(k+1)\n",
        "            beta  = checkpt[\"beta\"]\n",
        "            diag  = 0.25 * (beta + Jgen)\n",
        "            uk    = gamma(k) * u1 + (1-gamma(k)) * (uk - diag * nablaJ)\n",
        "    proj = uk.detach().numpy()\n",
        "    return np.transpose(proj)\n",
        "#-------------------------------------------------------------------------------\n",
        "# Define the fixed quantities for the toy example\n",
        "#-------------------------------------------------------------------------------\n",
        "A  = np.array([[1, 2]])\n",
        "z1 = np.array([[1.0], [2.0]])\n",
        "b  = np.array([[2.25]])\n",
        "c  = np.array([[2.0], [0.0]])\n",
        "L  = np.linalg.norm(np.matmul(np.transpose(A),A))\n",
        "#-------------------------------------------------------------------------------\n",
        "# Algorithm Parameters\n",
        "#-------------------------------------------------------------------------------\n",
        "tau     = 0.2\n",
        "alpha   = 1.0 / L \n",
        "n_iters = 20\n",
        "#-------------------------------------------------------------------------------\n",
        "# Arrays for Plotting Results\n",
        "#-------------------------------------------------------------------------------\n",
        "trajL2O        = np.zeros((n_iters,2))\n",
        "trajRPG        = np.zeros((n_iters,2))\n",
        "feasible_set1  = np.zeros((100, 2))\n",
        "feasible_set2  = np.zeros((100, 2))\n",
        "manifold       = np.zeros((100, 2))\n",
        "#-------------------------------------------------------------------------------\n",
        "# Algorithm Operators\n",
        "#-------------------------------------------------------------------------------\n",
        "def grad_H(u): # gradient of least squares\n",
        "    return np.matmul(np.transpose(A), np.matmul(A,u) - b)\n",
        "def proj_M(u): # projection onto M\n",
        "    return c + 0.75*(u - c) / np.linalg.norm(u - c)\n",
        "#-------------------------------------------------------------------------------\n",
        "# Analytic Projected Gradient\n",
        "#-------------------------------------------------------------------------------\n",
        "zt = z1\n",
        "trajRPG[0, :] = np.transpose(zt)\n",
        "for t in range(n_iters - 1):\n",
        "    yt = zt - alpha * grad_H(zt)\n",
        "    zt = (1 - tau) * zt + tau * proj_M(yt)\n",
        "    trajRPG[t+1, :] = np.transpose(zt)\n",
        "#-------------------------------------------------------------------------------\n",
        "# Adversarially Projected Gradient\n",
        "#-------------------------------------------------------------------------------\n",
        "zt = z1\n",
        "trajL2O[0, :] = np.transpose(zt)\n",
        "for t in range(n_iters - 1):\n",
        "    yt = zt - alpha * grad_H(zt)\n",
        "    zt = (1 - tau) * zt + tau * proj_M_L2O(np.transpose(yt))\n",
        "    trajL2O[t+1, :] = np.transpose(zt)\n",
        "    if t%5 == 0:\n",
        "        print('Completed Step ', t)\n",
        "#-------------------------------------------------------------------------------\n",
        "# Create Feasible Set and Manifold\n",
        "#-------------------------------------------------------------------------------\n",
        "for i in range(100):\n",
        "    temp = (1 - i / 100) * np.array([[0],[b[0,0]/2]]) + (i / 100.0) * np.array([[b[0,0]],[0]])\n",
        "    feasible_set1[i, :] = np.transpose(temp)\n",
        "    #temp = (1 - i / 100) * np.array([[0],[b[1,0]]]) + (i / 100.0) * np.array([[b[1,0]],[0]])\n",
        "    #feasible_set2[i, :] = np.transpose(temp)\n",
        "\n",
        "    temp = np.array([[2],[0]]) + 0.75 * np.array([[np.cos(i * 3.14/100)],[np.sin(i * 3.14/ 100)]])\n",
        "    manifold[i, :]  = np.transpose(temp)\n",
        "\n",
        "print('Iterations Complete.')\n",
        "\n",
        "filename = './rpg_adversarial.csv'\n",
        "with open(filename, 'w') as f: \n",
        "  f.write('a,b\\n')\n",
        "  for file_ctr in range(trajL2O.shape[0]):     \n",
        "    f.write('%0.5e,%0.5e\\n' % (trajL2O[file_ctr, 0], trajL2O[file_ctr, 1])) \n",
        "\n",
        "filename = './rpg_analytic.csv'\n",
        "with open(filename, 'w') as f: \n",
        "  f.write('a,b\\n')\n",
        "  for file_ctr in range(trajRPG.shape[0]):     \n",
        "    f.write('%0.5e,%0.5e\\n' % (trajRPG[file_ctr, 0], trajRPG[file_ctr, 1]))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HJooLWwV5vl"
      },
      "source": [
        "**Plotting Example Results**\n",
        "\n",
        "Below we output results of applying the projection scheme to our toy problem (alongside trajectories for the analytic projection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVukZnFgEv2a"
      },
      "source": [
        "#-----------------------------------------------------------------\n",
        "# Create Plots\n",
        "#-----------------------------------------------------------------\n",
        "fig = plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot(feasible_set1[:,0],feasible_set1[:,1], c='g', alpha=0.75, linewidth=4,)\n",
        "plt.plot(manifold[:,0],manifold[:,1], c='r', alpha=0.3, linewidth=4,)\n",
        "plt.scatter(u_true[:,0].detach(), u_true[:,1].detach(), c='r', alpha=0.5)\n",
        "plt.plot(trajRPG[:,0], trajRPG[:,1], c='b', alpha=0.9)\n",
        "plt.plot(trajL2O[:,0], trajL2O[:,1], c='m', alpha=0.9)\n",
        "\n",
        "plt.ylim(-0.5, 2.5);\n",
        "plt.xlim(-0, 3)\n",
        "plt.legend(['feasible set', 'manifold',  'proj grad', 'adv proj grad', 'manifold samples', ], loc='upper right',)\n",
        "title_str = 'Trajectories over time'\n",
        "plt.title(title_str)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}